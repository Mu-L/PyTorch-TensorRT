diff --git a/sam2/modeling/backbones/image_encoder.py b/sam2/modeling/backbones/image_encoder.py
index 37e9266..13272fb 100644
--- a/sam2/modeling/backbones/image_encoder.py
+++ b/sam2/modeling/backbones/image_encoder.py
@@ -114,7 +114,7 @@ class FpnNeck(nn.Module):
             lateral_features = self.convs[n - i](x)
             if i in self.fpn_top_down_levels and prev_features is not None:
                 top_down_features = F.interpolate(
-                    prev_features.to(dtype=torch.float32),
+                    prev_features.to(dtype=torch.float16),
                     scale_factor=2.0,
                     mode=self.fpn_interp_model,
                     align_corners=(
diff --git a/sam2/modeling/position_encoding.py b/sam2/modeling/position_encoding.py
index 52ac226..6cd55ed 100644
--- a/sam2/modeling/position_encoding.py
+++ b/sam2/modeling/position_encoding.py
@@ -139,7 +139,7 @@ class PositionEmbeddingRandom(nn.Module):
         """Generate positional encoding for a grid of the specified size."""
         h, w = size
         device: Any = self.positional_encoding_gaussian_matrix.device
-        grid = torch.ones((h, w), device=device, dtype=torch.float32)
+        grid = torch.ones((h, w), device=device, dtype=torch.float16)
         y_embed = grid.cumsum(dim=0) - 0.5
         x_embed = grid.cumsum(dim=1) - 0.5
         y_embed = y_embed / h
@@ -155,7 +155,7 @@ class PositionEmbeddingRandom(nn.Module):
         coords = coords_input.clone()
         coords[:, :, 0] = coords[:, :, 0] / image_size[1]
         coords[:, :, 1] = coords[:, :, 1] / image_size[0]
-        return self._pe_encoding(coords.to(torch.float))  # B x N x C
+        return self._pe_encoding(coords.to(torch.float16))  # B x N x C
 
 
 # Rotary Positional Encoding, adapted from:
diff --git a/sam2/modeling/sam/prompt_encoder.py b/sam2/modeling/sam/prompt_encoder.py
index 6b3bbb9..a13f892 100644
--- a/sam2/modeling/sam/prompt_encoder.py
+++ b/sam2/modeling/sam/prompt_encoder.py
@@ -86,18 +86,42 @@ class PromptEncoder(nn.Module):
         points = points + 0.5  # Shift to center of pixel
         if pad:
             padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)
-            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)
+            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device, dtype=torch.int)
+            # padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)
             points = torch.cat([points, padding_point], dim=1)
-            labels = torch.cat([labels, padding_label], dim=1)
+            labels = torch.cat([labels, padding_label], dim=1) # labels.to(torch.int)
         point_embedding = self.pe_layer.forward_with_coords(
             points, self.input_image_size
         )
-        point_embedding[labels == -1] = 0.0
-        point_embedding[labels == -1] += self.not_a_point_embed.weight
-        point_embedding[labels == 0] += self.point_embeddings[0].weight
-        point_embedding[labels == 1] += self.point_embeddings[1].weight
-        point_embedding[labels == 2] += self.point_embeddings[2].weight
-        point_embedding[labels == 3] += self.point_embeddings[3].weight
+
+        # Make int mask for each label condition and use index_select instead of index_put
+        mask_neg1 = (labels == -1).to(torch.int)
+        mask_0 = (labels == 0).to(torch.int)
+        mask_1 = (labels == 1).to(torch.int)
+        mask_2 = (labels == 2).to(torch.int)
+        mask_3 = (labels == 3).to(torch.int)
+        
+        # Use broadcasting-compatible indexing
+        point_embedding = point_embedding * (1 - mask_neg1[:, :, None]) + self.not_a_point_embed.weight * mask_neg1[:, :, None]
+        point_embedding = point_embedding + self.point_embeddings[0].weight * mask_0[:, :, None]
+        point_embedding = point_embedding + self.point_embeddings[1].weight * mask_1[:, :, None]
+        point_embedding = point_embedding + self.point_embeddings[2].weight * mask_2[:, :, None]
+        point_embedding = point_embedding + self.point_embeddings[3].weight * mask_3[:, :, None]
+
+        # point_embedding[(labels == -1).to(torch.int)] = 0.0
+        # point_embedding[(labels == -1).to(torch.int)] += self.not_a_point_embed.weight
+        # point_embedding[(labels == 0).to(torch.int)] += self.point_embeddings[0].weight
+        # point_embedding[(labels == 1).to(torch.int)] += self.point_embeddings[1].weight
+        # point_embedding[(labels == 2).to(torch.int)] += self.point_embeddings[2].weight
+        # point_embedding[(labels == 3).to(torch.int)] += self.point_embeddings[3].weight
+        
+
+        # point_embedding[labels == -1] = 0.0
+        # point_embedding[labels == -1] += self.not_a_point_embed.weight
+        # point_embedding[labels == 0] += self.point_embeddings[0].weight
+        # point_embedding[labels == 1] += self.point_embeddings[1].weight
+        # point_embedding[labels == 2] += self.point_embeddings[2].weight
+        # point_embedding[labels == 3] += self.point_embeddings[3].weight
         return point_embedding
 
     def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:
@@ -161,16 +185,35 @@ class PromptEncoder(nn.Module):
             Bx(embed_dim)x(embed_H)x(embed_W)
         """
         bs = self._get_batch_size(points, boxes, masks)
-        sparse_embeddings = torch.empty(
-            (bs, 0, self.embed_dim), device=self._get_device()
-        )
+
+        sparse_embeddings = None
+
         if points is not None:
             coords, labels = points
             point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))
-            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)
+            sparse_embeddings = point_embeddings  
+
         if boxes is not None:
             box_embeddings = self._embed_boxes(boxes)
-            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)
+            if sparse_embeddings is None:
+                sparse_embeddings = box_embeddings  
+            else:
+                sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)
+
+        if sparse_embeddings is None:
+            sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())
+            
+        # sparse_embeddings = torch.empty(
+        #     (bs, 0, self.embed_dim), device=self._get_device()
+        # )
+        # if points is not None:
+        #     coords, labels = points
+        #     point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))
+        #     sparse_embeddings = torch.cat([sparse_embeddings[:, :0], point_embeddings], dim=1)
+
+        # if boxes is not None:
+        #     box_embeddings = self._embed_boxes(boxes)
+        #     sparse_embeddings = torch.cat([sparse_embeddings[:, :0], box_embeddings], dim=1)
 
         if masks is not None:
             dense_embeddings = self._embed_masks(masks)
diff --git a/sam2/modeling/sam/transformer.py b/sam2/modeling/sam/transformer.py
index b5b6fa2..4e4bb73 100644
--- a/sam2/modeling/sam/transformer.py
+++ b/sam2/modeling/sam/transformer.py
@@ -263,13 +263,12 @@ class Attention(nn.Module):
         k = self._separate_heads(k, self.num_heads)
         v = self._separate_heads(v, self.num_heads)
 
+        # Attention without kernel context for TensorRT compatibility
         dropout_p = self.dropout_p if self.training else 0.0
-        # Attention
         try:
-            with sdp_kernel_context(dropout_p):
-                out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)
+            # Directly call scaled_dot_product_attention without sdp_kernel_context
+            out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)
         except Exception as e:
-            # Fall back to all kernels if the Flash attention kernel fails
             warnings.warn(
                 f"Flash Attention kernel failed due to: {e}\nFalling back to all available "
                 f"kernels for scaled_dot_product_attention (which may have a slower speed).",
diff --git a/sam2/modeling/sam2_utils.py b/sam2/modeling/sam2_utils.py
index e16caae..2374f20 100644
--- a/sam2/modeling/sam2_utils.py
+++ b/sam2/modeling/sam2_utils.py
@@ -144,13 +144,30 @@ class LayerNorm2d(nn.Module):
         self.weight = nn.Parameter(torch.ones(num_channels))
         self.bias = nn.Parameter(torch.zeros(num_channels))
         self.eps = eps
+        self.num_channels = num_channels
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
-        u = x.mean(1, keepdim=True)
-        s = (x - u).pow(2).mean(1, keepdim=True)
-        x = (x - u) / torch.sqrt(s + self.eps)
-        x = self.weight[:, None, None] * x + self.bias[:, None, None]
+        # (N, C, H, W) -> (N, H, W, C)로 변환
+        x = x.permute(0, 2, 3, 1)
+        # layer_norm 적용
+        x = F.layer_norm(
+            x, 
+            normalized_shape=(self.num_channels,), 
+            weight=self.weight, 
+            bias=self.bias, 
+            eps=self.eps
+        )
+        # (N, H, W, C) -> (N, C, H, W)로 다시 변환
+        x = x.permute(0, 3, 1, 2)
         return x
+    
+    # def forward(self, x: torch.Tensor) -> torch.Tensor:
+    #     # u = x.mean(1, keepdim=True)
+    #     u = x.sum(1, keepdim=True) / x.size(1)
+    #     s = (x - u).pow(2).mean(1, keepdim=True)
+    #     x = (x - u) / torch.sqrt(s + self.eps)
+    #     x = self.weight[:, None, None] * x + self.bias[:, None, None]
+    #     return x
 
 
 def sample_box_points(
